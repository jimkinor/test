{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the K-Means analysis for the IBM Applied Data Science Capstone. This Notebook accompanies the the final project report which is provided in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Install dependencies requiring pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install BeautifulSoup4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geocoder -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: failed\n",
      "\n",
      "CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://repo.anaconda.com/pkgs/free/linux-64/repodata.json.bz2>\n",
      "Elapsed: -\n",
      "\n",
      "An HTTP error occurred when trying to retrieve this URL.\n",
      "HTTP errors are often intermittent, and a simple retry will get you on your way.\n",
      "\n",
      "If your current network has https://www.anaconda.com blocked, please file\n",
      "a support request with your network engineering team.\n",
      "\n",
      "SSLError(MaxRetryError('HTTPSConnectionPool(host=\\'repo.anaconda.com\\', port=443): Max retries exceeded with url: /pkgs/free/linux-64/repodata.json.bz2 (Caused by SSLError(\"Can\\'t connect to HTTPS URL because the SSL module is not available.\"))'))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4a633c83d294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeocoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNominatim\u001b[0m \u001b[0;31m# convert an address into latitude and longitude values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeocoder\u001b[0m \u001b[0;31m# to get coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopy'"
     ]
    }
   ],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "import geocoder # to get coordinates\n",
    "\n",
    "import requests # library to handle requests\n",
    "from bs4 import BeautifulSoup # library to parse HTML and XML documents\n",
    "\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import folium # map rendering library\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define FourSquare Credentials and Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your credentails:\n",
      "CLIENT_ID: SZZA5NOLNSMV4XLJAEVQXUWB2A3K0YDOAS3KKBCQ2RTUPJT0\n",
      "CLIENT_SECRET:KOLQQCRIG4WZMMAYQKHDA2KN1LFB4PZTH3JQ3Q2IGERW4UJR\n"
     ]
    }
   ],
   "source": [
    "CLIENT_ID = 'SZZA5NOLNSMV4XLJAEVQXUWB2A3K0YDOAS3KKBCQ2RTUPJT0' # your Foursquare ID\n",
    "CLIENT_SECRET = 'KOLQQCRIG4WZMMAYQKHDA2KN1LFB4PZTH3JQ3Q2IGERW4UJR' # your Foursquare Secret\n",
    "VERSION = '20180604'\n",
    "LIMIT = 30\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get Data - Portland neighborhood names sourced from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arlington Heights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goose Hollow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Neighborhood\n",
       "0  Arlington Heights\n",
       "1        Forest Park\n",
       "2       Goose Hollow"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoods = pd.read_csv(\"portland_neighborhoods.csv\")\n",
    "print(hoods.shape)\n",
    "hoods.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get geographical coordinates for the neighborhoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get coordinates\n",
    "def get_latlng(neighborhood):\n",
    "    # initialize your variable to None\n",
    "    lat_lng_coords = None\n",
    "    # loop until you get the coordinates\n",
    "    while(lat_lng_coords is None):\n",
    "        g = geocoder.arcgis('{}, Portland, Oregon'.format(neighborhood))\n",
    "        lat_lng_coords = g.latlng\n",
    "    return lat_lng_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function to get the coordinates, store in a new list using list comprehension\n",
    "coords = [get_latlng(neighborhood) for neighborhood in hoods[\"Neighborhood\"].tolist()]\n",
    "print(len(coords))\n",
    "coords[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary dataframe to populate the coordinates into Latitude and Longitude\n",
    "df_coords = pd.DataFrame(coords, columns=['Latitude', 'Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the coordinates into the original dataframe\n",
    "hoods['Latitude'] = df_coords['Latitude']\n",
    "hoods['Longitude'] = df_coords['Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the neighborhoods and the coordinates\n",
    "print(hoods.shape)\n",
    "hoods.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame as CSV file\n",
    "hoods.to_csv(\"hoods.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a map centered on Portland, OR with neighborhoods superimposed on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the coordinates of Portland\n",
    "address = 'Portland, Oregon'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"my-application\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of Portland, Oregon {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map of Portland using latitude and longitude values\n",
    "portland_map = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, neighborhood in zip(hoods['Latitude'], hoods['Longitude'], hoods['Neighborhood']):\n",
    "    label = '{}'.format(neighborhood)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7).add_to(portland_map)\n",
    "\n",
    "portland_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the map as HTML file\n",
    "portland_map.save('portland_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lat, lng)\n",
    "hoods.head()\n",
    "hoods.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use FourSquare API to explore the neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 1000\n",
    "LIMIT = 100\n",
    "\n",
    "venues = []\n",
    "\n",
    "for lat, long, neighborhood in zip(hoods['Latitude'], hoods['Longitude'], hoods['Neighborhood']):\n",
    "    \n",
    "    # create the API request URL\n",
    "    url = \"https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}\".format(\n",
    "        CLIENT_ID,\n",
    "        CLIENT_SECRET,\n",
    "        VERSION,\n",
    "        lat,\n",
    "        long,\n",
    "        radius, \n",
    "        LIMIT)\n",
    "    \n",
    "    # make the GET request\n",
    "    results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "    \n",
    "    # return only relevant information for each nearby venue\n",
    "    for venue in results:\n",
    "        venues.append((\n",
    "            neighborhood,\n",
    "            lat, \n",
    "            long, \n",
    "            venue['venue']['name'], \n",
    "            venue['venue']['location']['lat'], \n",
    "            venue['venue']['location']['lng'],  \n",
    "            venue['venue']['categories'][0]['name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the venues list into a new DataFrame\n",
    "venues_df = pd.DataFrame(venues)\n",
    "\n",
    "# define the column names\n",
    "venues_df.columns = ['Neighborhoods', 'Latitude', 'Longitude', 'VenueName', 'VenueLatitude', 'VenueLongitude', 'VenueCategory']\n",
    "\n",
    "print(venues_df.shape)\n",
    "venues_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count how many venues for each neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_count = venues_df.groupby([\"Neighborhoods\"]).count()\n",
    "print(venue_count.shape)\n",
    "venue_count['VenueName'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the number of unique categories within the venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} uniques categories.'.format(len(venues_df['VenueCategory'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the list of categories\n",
    "venues_df['VenueCategory'].unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyze the Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "pdx_onehot = pd.get_dummies(venues_df[['VenueCategory']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "pdx_onehot['Neighborhoods'] = venues_df['Neighborhoods'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [pdx_onehot.columns[-1]] + list(pdx_onehot.columns[:-1])\n",
    "pdx_onehot = pdx_onehot[fixed_columns]\n",
    "\n",
    "print(pdx_onehot.shape)\n",
    "pdx_onehot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdx_grouped = pdx_onehot.groupby([\"Neighborhoods\"]).count().reset_index()\n",
    "pdx_grouped = pdx_onehot.groupby([\"Neighborhoods\"]).mean().reset_index()\n",
    "\n",
    "print(pdx_grouped.shape)\n",
    "pdx_grouped.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's use boxplots to evaluate the distributions of the different features from the Foursquare App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplotsa = pdx_grouped.drop(['Zoo Exhibit', 'Trail', 'Park','Nature Preserve', 'Hotel', 'Home Service', 'Golf Course', 'Garden', 'Forest', 'Dog Run', 'Disc Golf'], axis=1)\n",
    "boxplotsa.boxplot(vert=False, figsize=(10,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplotsb = pdx_grouped[['Zoo Exhibit', 'Trail', 'Park','Nature Preserve', 'Hotel', 'Home Service', 'Golf Course', 'Garden', 'Forest', 'Dog Run', 'Disc Golf']]\n",
    "boxplotsb.boxplot(vert=False, figsize=(10,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import Portland Demographics by Neighborhood Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_html(requests.get(\"https://www.pdxmonthly.com/articles/2017/3/22/portland-neighborhoods-by-the-numbers-2017-the-city\").content)[-1].to_csv(\"portland_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a dataframe for the Portland neighborhood demographics and real estate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_data = pd.read_csv(\"portland_data.csv\")\n",
    "print(pdx_data.shape)\n",
    "pdx_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get rid of columns that are redundant or irrelevant to the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_info_columns = [\"Unnamed: 0\", \"Adjusted population\", \"Minutes by car to downtown (est.)\", \"Parks and natural areas (Acres)\", \"Average home sale price ($)\", \"1-year median price change (2015-2016) (%)\", \"Distressed property sales (%)\", \"Married (except separated) (%)\", \"Divorced (%)\", \"Widowed (%)\", \"Separated (%)\", \"Never Married (%)\", \"Aggravated assault\", \"Arson\", \"Burglary\", \"Homicide\", \"Larceny\", \"Rape\", \"Robbery\", \"Vehicle thefts\", \"Nonviolent crimes\", \"Violent crimes\", \"Crimes per 1,000 residents\", \"Miles of bike routes (lanes, boulevards, multiuse paths)\", \"Miles of bike routes (all) per square mile\", \"Number of Max/streetcar lines\", \"Number of TriMet bus lines\"]\n",
    "pdx_data_clean = pdx_data.drop(no_info_columns, axis=1)\n",
    "pdx_data_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_data_clean.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's a \"Total\" column buried in the dataframe from the original source, get rid of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_data_clean = pdx_data_clean.drop([95])\n",
    "print(pdx_data_clean.shape)\n",
    "pdx_data_clean.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"Year built (avg.)\" column needs to be addressed because it is not meaningful to compare numbers like \"1998\" and \"2010\", the difference is miniscule. However, it is meaningful to compare the homes based on age, so create a new column based on a calculation of the building age using the current year and then drop the original column from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_data_clean = pdx_data_clean.drop(\"Year built (avg.)\", axis=1)\n",
    "pdx_data_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's use boxplots to evaluate the distributions of the different features from the Real Estate / Demographic data. Population moved from abroad...', 'Population in different state...', and 'Population in different country...' don't add any value as the distribution is very narrow, we will remove those from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots1 = pdx_data_clean.drop(['Median age', 'Number of transit lines (bus/MAX/streetcar)', 'Days on market (avg.)', '5-year median price change (2012-2016) (%)', 'Walk score', 'Homes sold in 2016 (#)', 'Average cost per square foot ($)', 'Median household income ($)', 'Median home sale price ($)', 'Renters - median monthly housing expenses ($)', 'Adjusted population density (people per sq. mi., excluding parks and industrial tracts)'], axis=1)\n",
    "boxplots1.boxplot(vert=False, figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots2 = pdx_data_clean[['Homes sold in 2016 (#)']]\n",
    "boxplots2.boxplot(vert=False, figsize=(10, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merge the two datasets, Foursquare (pdx_grouped) and the real estate data from Portland Monthly (pdx_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots3 = pdx_data_clean[['Median home sale price ($)']]\n",
    "boxplots3.boxplot(vert=False, figsize=(10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots5 = pdx_data_clean[['Renters - median monthly housing expenses ($)']]\n",
    "boxplots5.boxplot(vert=False, figsize=(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots6 = pdx_data_clean[['Walk score']]\n",
    "boxplots6.boxplot(vert=False, figsize=(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots7 = pdx_data_clean[['5-year median price change (2012-2016) (%)']]\n",
    "boxplots7.boxplot(vert=False, figsize=(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots8 = pdx_data_clean[['Days on market (avg.)']]\n",
    "boxplots8.boxplot(vert=False, figsize=(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots9 = pdx_data_clean[['Average cost per square foot ($)']]\n",
    "boxplots9.boxplot(vert=False, figsize=(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots11 = pdx_data_clean[['Number of transit lines (bus/MAX/streetcar)']]\n",
    "boxplots11.boxplot(vert=False, figsize=(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots12 = pdx_data_clean[['Adjusted population density (people per sq. mi., excluding parks and industrial tracts)']]\n",
    "boxplots12.boxplot(vert=False, figsize=(10, 1))\n",
    "boxplots13 = pdx_data_clean[['Median household income ($)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_grouped.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop 3 columns based on narrow distribution in dataset\n",
    "pdx_data_clean = pdx_data_clean.drop(['Population in different county last year (%)', 'Population moved from abroad last year (%)', 'Population in different state last year (%)'], axis=1)\n",
    "\n",
    "#pdx_grouped pdx_data_clean\n",
    "pdx_combined = pd.concat([pdx_grouped, pdx_data_clean], join='inner', axis=1)\n",
    "print(pdx_combined.shape)\n",
    "pdx_combined.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get rid of the string columns to prepare for normalization and clustering, but first make a copy of the column with \"Neighborhoods\" to be merged back in later so we can then merge the geospatial data for mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_nbhd_column = pdx_combined[['Neighborhoods']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_clustering = pdx_combined.drop(['Neighborhoods', 'Neighborhood'], 1)\n",
    "pdx_column_names = pdx_clustering.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_clustering.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normalize the data to improve the outputs of the algorithm by having every feature on a common scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x, where x the 'scores' column's values as floats\n",
    "x = pdx_clustering.values.astype(float)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "pdx_normalized = pd.DataFrame(x_scaled)\n",
    "print(pdx_normalized.shape)\n",
    "pdx_normalized.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 12\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(pdx_normalized)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_normalized.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add the clustering labels to a new dataframe, rename the columns to their original names, and insert the 'Neighborhood' column back into position index=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe that includes the clusters.\n",
    "pdx_merged = pdx_normalized.copy()\n",
    "\n",
    "# add column names back in\n",
    "pdx_merged.columns = pdx_column_names\n",
    "\n",
    "pdx_merged.insert(0,'Neighborhood', pdx_nbhd_column)\n",
    "#pdx_merged['Neighborhood'] = pdx_nbhd_column\n",
    "\n",
    "# add clustering labels\n",
    "pdx_merged[\"Cluster Labels\"] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdx_merged.shape)\n",
    "pdx_merged.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge neighborhood geo data with the k-means data to add latitude/longitude for each neighborhood\n",
    "pdx_merged = pdx_merged.join(hoods.set_index(\"Neighborhood\"), on=\"Neighborhood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i+x+(i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.jet(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(pdx_merged['Latitude'], pdx_merged['Longitude'], pdx_merged['Neighborhood'], pdx_merged['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' - Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pdx_merged.groupby('Cluster Labels').mean()\n",
    "summary_df = summary_df.drop('Latitude', axis=1)\n",
    "summary_df = summary_df.drop('Longitude', axis=1)\n",
    "print(summary_df.shape)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check to see how many records for each Cluster Label by counting the records for any one of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdx_merged_ct = pdx_merged.groupby('Cluster Labels').count()\n",
    "pdx_merged_ct['ATM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bar Charts showing top 20 category scores for each cluster. The data is sorted in ascending order by feature score so we will plot only the last 20 entries of the dataframe. These charts provide the most distinguishing characteristics attributed to the neighborhood cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a variable for setting the range for charting last 20 records\n",
    "records_to_plot = -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot bar charts for each of the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[0]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[1]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[2]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[3]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[4]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[5]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[6]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[7]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[8]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[9]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[10]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = summary_df.iloc[11]\n",
    "row.sort_values(axis = 0, ascending = True, inplace = True)\n",
    "row = row[records_to_plot:]\n",
    "#summary_df = summary_df.sort_values(row)\n",
    "row.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
